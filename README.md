# Long-text-Sentiment-Evaluation
We provide a way to evaluate LLMs' ability of long-text sentiment analysis

First, you need to replace all the API keys in the LLMS.py file with your own. If you want to test other large models, please add the API interfaces in LLMS.py yourself.

Please note that some large model platforms may require top-up before use, to avoid any interruptions during usage.

Replace the file paths in main.py with your own.

Replace the model in main.py with the large model you want to test.

Since some of the test questions are subject to copyright, we are unable to make them public. This evaluation code only provides example tests to help you understand the approach to long-text sentiment analysis evaluation.

 If you want to view the results of each test case individually, you can modify the code yourself. This code is responsible for automatically calculating the accuracy.
